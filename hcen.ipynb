{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Param\n",
    "# test = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test = True\n",
    "if test == \"False\" or test == False:\n",
    "    is_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e60a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db3c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import DataLoader\n",
    "from utils.data_io import join_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cc2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2520ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data_loader.get_data_train()\n",
    "test_df = data_loader.get_data_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ffb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc = data_loader.get_data_desc()\n",
    "\n",
    "label_column = data_desc['label_column']\n",
    "text_column = data_desc['text_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76d9830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coronary', 'myocardial', 'hypertension', 'cardiac', 'systolic', 'colitis', 'esophageal', 'gastrointestinal', 'bowel', 'duodenal', 'defect', 'loss', 'airway', 'graft', 'respiratory', 'cancer', 'carcinoma', 'sarcoma', 'malignancy', 'chemotherapy', 'brain', 'cerebral', 'neuronal', 'motor', 'cord']\n"
     ]
    }
   ],
   "source": [
    "keyword_concepts = data_loader.get_keyword_concepts()\n",
    "keywords = []\n",
    "for k in keyword_concepts.keys():\n",
    "    keywords += keyword_concepts[k]\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a05ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cardiac Function and Disorders',\n",
       " 'Heart Muscle and Blood Pressure',\n",
       " 'Coronary Artery Issues',\n",
       " 'Intestinal and Esophageal Conditions',\n",
       " 'Gastrointestinal Tract Ailments',\n",
       " 'Inflammatory Bowel Diseases',\n",
       " 'General Pathological States',\n",
       " 'Respiratory System Impairments',\n",
       " 'Tissue and Graft Issues',\n",
       " 'Malignant Tumors and Growths',\n",
       " 'Cancer Treatment and Types',\n",
       " 'Oncological Malignancies',\n",
       " 'Central and Peripheral Nervous System Disorders',\n",
       " 'Brain and Cerebral Conditions',\n",
       " 'Spinal Cord and Motor Function Impairment']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_concepts = data_loader.get_abstract_concepts()\n",
    "abstract_concepts = [ac['abstract_concept_name'] for ac in abstract_concepts]\n",
    "abstract_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc8e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1797b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_df[label_column])\n",
    "train_df[label_column] = le.transform(train_df[label_column])\n",
    "test_df[label_column] = le.transform(test_df[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7db2b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cardiovascular diseases',\n",
       " 'digestive system diseases',\n",
       " 'general pathological conditions',\n",
       " 'neoplasms',\n",
       " 'nervous system diseases']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(le.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ce8535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    train_df = train_df.groupby(label_column).sample(1)\n",
    "    test_df = test_df.groupby(label_column).sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598efcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df[text_column].to_list()\n",
    "train_labels = train_df[label_column].to_list()\n",
    "test_texts = test_df[text_column].to_list()\n",
    "test_labels = test_df[label_column].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef67e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a52883",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import networkx as nx\n",
    "from matplotlib import cm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b661721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    model = BertForSequenceClassification.from_pretrained(path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "class ConceptNetwork(nn.Module):\n",
    "    def __init__(self, concept_names, embedding_dim, keyword_nli_model_path, abstract_nli_model_path):\n",
    "        \"\"\"\n",
    "        concept_names: List[List[str]] gồm [keywords, abstract_concepts, labels]\n",
    "        \"\"\"\n",
    "        super(ConceptNetwork, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.concept_names = concept_names  # [[keywords], [abstracts], [labels]]\n",
    "        self.sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # === Embeddings ===\n",
    "        self.keyword_embeddings = nn.Parameter(torch.randn(len(concept_names[0]), embedding_dim))\n",
    "        self.abstract_embeddings = nn.Parameter(torch.randn(len(concept_names[1]), embedding_dim))\n",
    "        self.label_embeddings = nn.Parameter(torch.randn(len(concept_names[2]), embedding_dim))\n",
    "\n",
    "        # === Beta parameters (learnable reliability weights) ===\n",
    "        self.keyword_betas = nn.Parameter(torch.zeros(len(concept_names[0])))\n",
    "        self.abstract_betas = nn.Parameter(torch.zeros(len(concept_names[1])))\n",
    "\n",
    "        # === Semantic predictor (shared) ===\n",
    "        self.semantic_predictor = nn.Linear(embedding_dim, 384)  # SBERT output dim\n",
    "\n",
    "        # === Load NLI scorers ===\n",
    "        self.keyword_scorer, self.keyword_tokenizer = load_model(keyword_nli_model_path)\n",
    "        self.abstract_scorer, self.abstract_tokenizer = load_model(abstract_nli_model_path)\n",
    "\n",
    "    def forward(self, texts, device):\n",
    "        \"\"\"\n",
    "        texts: List[str] – batch of input texts\n",
    "        device: torch.device\n",
    "        Returns:\n",
    "            predictions: [B, n_labels]\n",
    "            keyword_semantic, abstract_semantic, label_semantic: [n_i, 384]\n",
    "            keyword_scores: [B, n_keywords]\n",
    "            abstract_scores: [B, n_abstracts]\n",
    "        \"\"\"\n",
    "        B = len(texts)\n",
    "        device = torch.device(device)\n",
    "\n",
    "        # === Keyword layer: direct logits ===\n",
    "        keyword_scores = []\n",
    "        for cname in self.concept_names[0]:\n",
    "            inputs = self.keyword_tokenizer(texts, [cname] * B,\n",
    "                                            return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.keyword_scorer(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                scores = probs[:, 1]  # entailment\n",
    "            keyword_scores.append(scores)\n",
    "        keyword_scores = torch.stack(keyword_scores, dim=1)  # [B, n_kw]\n",
    "\n",
    "        # === Abstract layer: direct logits ===\n",
    "        abstract_direct = []\n",
    "        for cname in self.concept_names[1]:\n",
    "            inputs = self.abstract_tokenizer(texts, [cname] * B,\n",
    "                                             return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.abstract_scorer(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                scores = probs[:, 1]\n",
    "            abstract_direct.append(scores)\n",
    "        abstract_direct = torch.stack(abstract_direct, dim=1)  # [B, n_abs]\n",
    "\n",
    "        # === Relation logits: keyword → abstract ===\n",
    "        attn_kw_abs = torch.matmul(self.keyword_embeddings, self.abstract_embeddings.T)  # [n_kw, n_abs]\n",
    "        attn_kw_abs = torch.softmax(attn_kw_abs, dim=0)  # softmax theo n_kw\n",
    "        relation_kw_abs = torch.matmul(keyword_scores, attn_kw_abs)  # [B, n_abs]\n",
    "\n",
    "        # === Abstract scores: combine direct + relation ===\n",
    "        abstract_betas = torch.sigmoid(self.abstract_betas)  # [n_abs]\n",
    "        abstract_scores = abstract_direct * abstract_betas + relation_kw_abs * (1 - abstract_betas)  # [B, n_abs]\n",
    "\n",
    "        # === Label layer: relation only (abstract → label) ===\n",
    "        attn_abs_lbl = torch.matmul(self.abstract_embeddings, self.label_embeddings.T)  # [n_abs, n_lbl]\n",
    "        attn_abs_lbl = torch.softmax(attn_abs_lbl, dim=0)  # softmax theo n_abs\n",
    "        predictions = torch.matmul(abstract_scores, attn_abs_lbl)  # [B, n_lbl]\n",
    "\n",
    "        # === Semantic predictions ===\n",
    "        keyword_semantic = self.semantic_predictor(self.keyword_embeddings)     # [n_kw, 384]\n",
    "        abstract_semantic = self.semantic_predictor(self.abstract_embeddings)   # [n_abs, 384]\n",
    "        label_semantic = self.semantic_predictor(self.label_embeddings)         # [n_lbl, 384]\n",
    "\n",
    "        return predictions, keyword_semantic, abstract_semantic, label_semantic, keyword_scores, abstract_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa627de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, train_texts, train_labels, val_texts, val_labels, concept_names, sbert_embeddings,\n",
    "                batch_size=16, num_epochs=100, patience=5, lambda_semantic=1.0, device='cuda'):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    # Chuyển SBERT embeddings sang tensor 1 lần duy nhất\n",
    "    sbert_tensors = [\n",
    "        torch.tensor(arr, device=device, dtype=torch.float32)\n",
    "        for arr in sbert_embeddings\n",
    "    ]\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "\n",
    "        for i in tqdm(range(0, len(train_texts), batch_size), desc=f\"Epoch {epoch+1}\"):\n",
    "            batch_texts = train_texts[i:i + batch_size]\n",
    "            batch_labels = torch.tensor(train_labels[i:i + batch_size], dtype=torch.long).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions, keyword_semantic, abstract_semantic, label_semantic, _, _ = model(batch_texts, device)\n",
    "\n",
    "            # Loss predictor\n",
    "            pred_loss = criterion(predictions, batch_labels)\n",
    "\n",
    "            # Semantic loss\n",
    "            semantic_loss = 0\n",
    "            for layer_idx, embeddings in enumerate([keyword_semantic, abstract_semantic, label_semantic]):\n",
    "                semantic_loss += mse_loss(embeddings, sbert_tensors[layer_idx])\n",
    "\n",
    "            total_loss = pred_loss + lambda_semantic * semantic_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += total_loss.item()\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(val_texts), batch_size):\n",
    "                batch_texts = val_texts[i:i + batch_size]\n",
    "                batch_labels = torch.tensor(val_labels[i:i + batch_size], dtype=torch.long).to(device)\n",
    "\n",
    "                predictions, keyword_semantic, abstract_semantic, label_semantic, _, _ = model(batch_texts, device)\n",
    "\n",
    "                pred_loss = criterion(predictions, batch_labels)\n",
    "                semantic_loss = 0\n",
    "                for layer_idx, embeddings in enumerate([keyword_semantic, abstract_semantic, label_semantic]):\n",
    "                    semantic_loss += mse_loss(embeddings, sbert_tensors[layer_idx])\n",
    "\n",
    "                val_loss += (pred_loss + lambda_semantic * semantic_loss).item()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / math.ceil(len(train_texts) / batch_size)\n",
    "        avg_val_loss = val_loss / math.ceil(len(val_texts) / batch_size)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"✅ Saved new best model.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"⏹️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # === Plot losses ===\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_texts, test_labels, concept_names, batch_size=16, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(test_texts), batch_size), desc=\"Testing\"):\n",
    "            batch_texts = test_texts[i:i+batch_size]\n",
    "            batch_labels = test_labels[i:i+batch_size]\n",
    "            \n",
    "            outputs, _, _, _, _, _ = model(batch_texts, device)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(batch_labels)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=concept_names[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ff87012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretable_prediction(model, test_text, test_label, concept_names, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"\\nInterpretable Prediction for Sample:\")\n",
    "    print(f\"Input Text: {test_text}\")\n",
    "    print(f\"True Label: {concept_names[2][test_label]}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process single sample\n",
    "        predictions, _, _, _, keyword_scores, abstract_scores = model([test_text], device)\n",
    "        \n",
    "        # Keyword layer activations\n",
    "        print(\"\\nKeyword Layer Activations:\")\n",
    "        for concept_name, score in zip(concept_names[0], keyword_scores[0]):\n",
    "            print(f\"  {concept_name}: {score:.4f}\")\n",
    "        \n",
    "        # Abstract layer activations\n",
    "        print(\"\\nAbstract Layer Activations:\")\n",
    "        for concept_name, score in zip(concept_names[1], abstract_scores[0]):\n",
    "            print(f\"  {concept_name}: {score:.4f}\")\n",
    "        \n",
    "        # Label layer predictions\n",
    "        label_probs = torch.softmax(predictions, dim=-1)[0]\n",
    "        print(\"\\nLabel Layer Probabilities:\")\n",
    "        for concept_name, prob in zip(concept_names[2], label_probs):\n",
    "            print(f\"  {concept_name}: {prob:.4f}\")\n",
    "        \n",
    "        # Final prediction\n",
    "        predicted_label_idx = torch.argmax(predictions, dim=-1).item()\n",
    "        print(f\"\\nPredicted Label: {concept_names[2][predicted_label_idx]}\")\n",
    "        \n",
    "        return keyword_scores[0].cpu().numpy(), abstract_scores[0].cpu().numpy(), label_probs.cpu().numpy(), predicted_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib import cm\n",
    "\n",
    "def visualize_network(model, concept_names, keyword_scores, abstract_scores, label_probs, predicted_label_idx, output_file='network_visualization.png'):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes\n",
    "    for i, kw in enumerate(concept_names[0]):\n",
    "        G.add_node(f\"K_{i}\", label=kw, layer='keyword', score=keyword_scores[i])\n",
    "    for i, abs_c in enumerate(concept_names[1]):\n",
    "        G.add_node(f\"A_{i}\", label=abs_c, layer='abstract', score=abstract_scores[i])\n",
    "    for i, lbl in enumerate(concept_names[2]):\n",
    "        G.add_node(f\"L_{i}\", label=lbl, layer='label', score=label_probs[i])\n",
    "\n",
    "    # Edges: keyword → abstract (use beta)\n",
    "    for i in range(len(concept_names[0])):\n",
    "        for j in range(len(concept_names[1])):\n",
    "            weight = model.keyword_betas[i].item() * model.abstract_betas[j].item()\n",
    "            G.add_edge(f\"K_{i}\", f\"A_{j}\", weight=weight)\n",
    "\n",
    "    # Edges: abstract → label (recompute attention)\n",
    "    with torch.no_grad():\n",
    "        abs_emb = model.abstract_embeddings.detach().cpu()\n",
    "        lbl_emb = model.label_embeddings.detach().cpu()\n",
    "        attn = torch.matmul(abs_emb, lbl_emb.T)  # [n_abs, n_lbl]\n",
    "        attn = torch.softmax(attn, dim=0)\n",
    "\n",
    "    for i in range(len(concept_names[1])):\n",
    "        for j in range(len(concept_names[2])):\n",
    "            weight = attn[i, j].item()\n",
    "            G.add_edge(f\"A_{i}\", f\"L_{j}\", weight=weight)\n",
    "\n",
    "    # Positioning\n",
    "    pos = {}\n",
    "    max_nodes = max(len(concept_names[0]), len(concept_names[1]), len(concept_names[2]))\n",
    "    for i in range(len(concept_names[0])):\n",
    "        pos[f\"K_{i}\"] = (0, max_nodes - i)\n",
    "    for i in range(len(concept_names[1])):\n",
    "        pos[f\"A_{i}\"] = (1, max_nodes - i)\n",
    "    for i in range(len(concept_names[2])):\n",
    "        pos[f\"L_{i}\"] = (2, max_nodes - i)\n",
    "\n",
    "    # Node visuals\n",
    "    node_colors = [data['score'] for _, data in G.nodes(data=True)]\n",
    "    node_sizes = [500 + data['score'] * 2000 for _, data in G.nodes(data=True)]\n",
    "    edge_widths = [abs(G[u][v]['weight']) * 2 for u, v in G.edges()]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, cmap=cm.viridis)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5)\n",
    "\n",
    "    labels = {n: f\"{d['label']}\\n{d['score']:.2f}\" for n, d in G.nodes(data=True)}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "\n",
    "    # Highlight predicted node\n",
    "    pred_node = f\"L_{predicted_label_idx}\"\n",
    "    pred_size = 500 + G.nodes[pred_node]['score'] * 2000\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[pred_node], node_color='red', node_size=pred_size)\n",
    "\n",
    "    plt.title(\"Concept Network Visualization\")\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc4b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = BertForSequenceClassification.from_pretrained(path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_nli_model_path = join_path(dataset, 'scorer_model', 'keyword_scorer')\n",
    "abstract_nli_model_path = join_path(dataset, 'scorer_model', 'abstract_scorer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f52748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT embeddings (precomputed for efficiency)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sbert_embeddings = [\n",
    "    sbert_model.encode(keywords, convert_to_numpy=True),\n",
    "    sbert_model.encode(abstract_concepts, convert_to_numpy=True),\n",
    "    sbert_model.encode(labels, convert_to_numpy=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549abd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_names = [keywords, abstract_concepts, labels]\n",
    "model = ConceptNetwork(\n",
    "    concept_names, embedding_dim=64, \n",
    "    keyword_nli_model_path=keyword_nli_model_path,\n",
    "    abstract_nli_model_path=abstract_nli_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd6fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99983eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4/4 [00:35<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.1116, Val Loss: 1.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:38<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.0388, Val Loss: 1.3118\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_texts, train_labels, val_texts, val_labels,\n",
    "    concept_names, sbert_embeddings, batch_size=16, num_epochs=20,\n",
    "    patience=5, lambda_semantic=0.1, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37f159be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "        cardiovascular diseases       0.00      0.00      0.00         1\n",
      "      digestive system diseases       0.00      0.00      0.00         1\n",
      "general pathological conditions       0.20      1.00      0.33         1\n",
      "                      neoplasms       0.00      0.00      0.00         1\n",
      "        nervous system diseases       0.00      0.00      0.00         1\n",
      "\n",
      "                       accuracy                           0.20         5\n",
      "                      macro avg       0.04      0.20      0.07         5\n",
      "                   weighted avg       0.04      0.20      0.07         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dauduchieu/Desktop/iSE-CBM/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/dauduchieu/Desktop/iSE-CBM/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/dauduchieu/Desktop/iSE-CBM/venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_texts, test_labels, concept_names, batch_size=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "260cb781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretable Prediction for Sample:\n",
      "Input Text: Plasma concentrations of epinephrine during CPR in the dog. STUDY OBJECTIVE: The purpose of this study was to evaluate whether the marked increase in the plasma concentrations of epinephrine during cardiopulmonary arrest and basic life support (BLS) could be due in part to decreased distribution and/or elimination. DESIGN AND INTERVENTIONS: Dogs were randomly assigned to undergo adrenalectomy or sham-operation. Some adrenalectomized animals received an epinephrine infusion. MEASUREMENTS AND MAIN RESULTS: In the seven sham-operated dogs, the plasma epinephrine concentrations increased markedly during BLS as expected. In the seven adrenalectomized dogs receiving a constant infusion of epinephrine, cardiopulmonary arrest and BLS induced a three to sixfold increase in plasma epinephrine concentrations, with an increase in the mean plasma epinephrine concentrations (calculated from the area under the curve) of 1.21 +/- 0.12 ng/mL (P less than .05). In the seven adrenalectomized dogs receiving a constant epinephrine infusion but not subjected to cardiopulmonary arrest, the plasma epinephrine concentrations remained stable. Finally, in the seven adrenalectomized dogs not receiving an epinephrine infusion, the mean plasma epinephrine concentrations during BLS (calculated from the area under the curve) increased only by 0.05 +/- 0.04 ng/mL, significantly less than in adrenalectomized dogs receiving an epinephrine infusion (P less than .01). CONCLUSION: The increase in plasma epinephrine concentrations during cardiopulmonary arrest and BLS is due in part to an altered disposition of epinephrine. \n",
      "True Label: cardiovascular diseases\n",
      "\n",
      "Keyword Layer Activations:\n",
      "  coronary: 0.5099\n",
      "  myocardial: 0.5118\n",
      "  hypertension: 0.5139\n",
      "  cardiac: 0.4982\n",
      "  systolic: 0.4195\n",
      "  colitis: 0.5269\n",
      "  esophageal: 0.4869\n",
      "  gastrointestinal: 0.5135\n",
      "  bowel: 0.5083\n",
      "  duodenal: 0.5052\n",
      "  defect: 0.5147\n",
      "  loss: 0.5184\n",
      "  airway: 0.5185\n",
      "  graft: 0.5049\n",
      "  respiratory: 0.5004\n",
      "  cancer: 0.5012\n",
      "  carcinoma: 0.4992\n",
      "  sarcoma: 0.5168\n",
      "  malignancy: 0.4569\n",
      "  chemotherapy: 0.5198\n",
      "  brain: 0.5037\n",
      "  cerebral: 0.5457\n",
      "  neuronal: 0.5045\n",
      "  motor: 0.5270\n",
      "  cord: 0.4998\n",
      "\n",
      "Abstract Layer Activations:\n",
      "  Cardiac Function and Disorders: 0.6692\n",
      "  Heart Muscle and Blood Pressure: 0.6345\n",
      "  Coronary Artery Issues: 0.6547\n",
      "  Intestinal and Esophageal Conditions: 0.6226\n",
      "  Gastrointestinal Tract Ailments: 0.6368\n",
      "  Inflammatory Bowel Diseases: 0.6285\n",
      "  General Pathological States: 0.6480\n",
      "  Respiratory System Impairments: 0.6255\n",
      "  Tissue and Graft Issues: 0.6304\n",
      "  Malignant Tumors and Growths: 0.6223\n",
      "  Cancer Treatment and Types: 0.4591\n",
      "  Oncological Malignancies: 0.5680\n",
      "  Central and Peripheral Nervous System Disorders: 0.6027\n",
      "  Brain and Cerebral Conditions: 0.6333\n",
      "  Spinal Cord and Motor Function Impairment: 0.6436\n",
      "\n",
      "Label Layer Probabilities:\n",
      "  cardiovascular diseases: 0.1090\n",
      "  digestive system diseases: 0.1776\n",
      "  general pathological conditions: 0.2900\n",
      "  neoplasms: 0.2391\n",
      "  nervous system diseases: 0.1843\n",
      "\n",
      "Predicted Label: general pathological conditions\n"
     ]
    }
   ],
   "source": [
    "# Interpretable prediction and visualization for a single sample\n",
    "if test_texts and test_labels:\n",
    "    keyword_scores, abstract_scores, label_probs, predicted_label_idx = interpretable_prediction(\n",
    "        model, test_texts[0], test_labels[0], concept_names, device=device\n",
    "    )\n",
    "    visualize_network(model, concept_names, keyword_scores, abstract_scores, label_probs, predicted_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13b4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
